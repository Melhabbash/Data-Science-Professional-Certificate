---
title: '**MovieLens Recommendation System Project** \newline'


subtitle: '**HarvardX Data Science Professional Certificate: PH125.9x**  \newline Data Science Initiative   \newline The Palestinian Central Bureau of Statistics (PCBS)   \newline  Arab American University of Palestine (AAUP) \newline'


author: 'Mohammed K. Elhabbash \newline '

date: "_`r format(Sys.Date(), '%d %B, %Y')`_"
output:
  pdf_document:
    df_print: kable
    toc: true
    toc_depth: 3
    fig_width: 12
    fig_height: 6

fontsize: 13pt
header-includes:
   - \usepackage[font={footnotesize,it}, labelfont={bf}]{caption}
   
include-before: '`\newpage{}`{=latex}'
urlcolor: blue
---

\newpage
# 1.Introduction

A MovieLens recommendation system is an automated system that attempts to anticipate a user's preference for a movie to watch, depending on the previous user's rating.
In general recommendation, systems are employed to suggest items. These items vary according to the field, for instance, books, news, research articles, search queries, movies, restaurants, etc. 
Suggested items are selected from a pre-prepared data set. The function of the recommendation system is to select item/s from this data set and recommend it/them to the user.
Famous companies such as Amazon, Twitter, Facebook, Netflix and Spotify rely on recommendation systems to promote their commodity and satisfy customers desires.\ 
The following report is a response to a mandatory assignment in the ninth and final course in HarvardX's multi-part  " Data Science Professional Certificate". The Project Instructions, and  Project Submission is given in appendix A, and appendix B respectively. 

##    1.1 Project methodology:
[10M Dataset](https://grouplens.org/datasets/movielens/10m/) will be split into two portions edx, and validation dataset.\ 
**edx dataset:** 90% of [10M Dataset](https://grouplens.org/datasets/movielens/10m/)
**validation dataset:** 10% of [10M Dataset](https://grouplens.org/datasets/movielens/10m/)

edx dataset will be cleaned, analysed and visualized to explore features of the dataset. Machine learning techniques and models will be employed to form a model for Movie suggestions.
Models will be built rely on the edx dataset.   The validation dataset then will be used to test the performance and effectiveness of the model depending on RMSE.


```{r setup, include=FALSE}
#########################################################################################################################
knitr::opts_chunk$set(echo = TRUE , warning = FALSE, message = FALSE,
                      fig.align="center", out.width="60%")
## Install required Package required -- from edx
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("rmarkdown", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")

## Install Additional Package  
## Used formattable and kableExtra Package to formate Table
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(formattable)) install.packages("formattable", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
library(kableExtra)
library(formattable)
library(recosystem)  ## used to make matrix factorization 
library(ggthemes)
library(lubridate) ## used to deal with timestamp
library(knitr)
library(rmarkdown)
library(dplyr)
set_theme <- theme(text = element_text(size=16), panel.border = element_rect(colour="black", linetype = "solid", fill=NA), plot.title = element_text(hjust = 0.5, size = 18), plot.caption = element_text(hjust = 0.5))
#########################################################################################################################
```


```{r partition-data, include=FALSE, echo=FALSE}
#########################################################################################################################
##  HarvardX in PH125.9x Data Science: Capstone Movielends project 
##  Create edx set, validation set (final hold-out test set)
#########################################################################################################################
# Note: this process could take a couple of minutes
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                           title = as.character(title),
#                                           genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
#head(movielens)
# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

##    1.2 Dataset:
[MovieLens Latest dataset](https://grouplens.org/datasets/movielens/latest/) is a large, continuous-updated dataset that involves the following features: 
\newline 
**Quantitative features.**

+ userId: an integer variable with a unique ID for each user. 
+ movieId: a numeric variable with a unique ID for each movie.  
+ timestamp: an integer variable represents the date and time of the ratting that was given to the movie.  
\newline
**Qualitative features.**

+ title:  character variable which names the movie (not unique). 
+ genres: character variable associated with the movie.   
\newline
**Outcome:**

+ rating: a numeric variable lay between 0.5 and 5 express the satisfaction of the user toward a specific movie.  (5 is the best value).   

The [10M Dataset](https://grouplens.org/datasets/movielens/10m/) is the used dataset in this project, which is last updated in Sep./2018.

### 1.2.1 edx dataset:

edx as a subset of the 10M MovieLens data set of 9,000,055 raws and 6 columns.  10677 movies were rated by 69878 users, where movies have 797 genres with an average rating of 3.512 The ratting process starts in 1995  and ends in 2009.

```{r Table1, echo=FALSE}
###### Table 1: Edx Data set summary 
edx_Dataset_summary <- data.frame(rows_number = nrow(edx),
                                  columns_number = ncol(edx),
                                  users_number= n_distinct(edx$userId),
                                  movies_number = n_distinct(edx$movieId),
                                  average_rating = round(mean(edx$rating),3),
                                  genres_number = n_distinct(edx$genres),
                                  first_rating_Date = as.Date(as.POSIXct(min(edx$timestamp), origin = "1970-01-01")),
                                  last_rating_date = as.Date(as.POSIXct(max(edx$timestamp), origin = "1970-01-01")))
edx_Dataset_summary %>% 
  kable(caption = "Summary of the edx data set", align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  row_spec(1:6, color =  "#41729F", bold = F) %>%
  kable_styling("hover",full_width = FALSE, position = "center", latex_options = c("scale_down", "hold_position"), font_size = 12)
```
\ 
The columns represent features i.e. userId,  movieId, rating, timestamp, title, and genres. Each user will take a unique Id. and each movie has also a unique Id. The user has the opportunity to rate the movies which he/she watched. The ratting is a number between 0.5 and 5 where 5 is the best. edx consist of 69,878 unique users and 10677 unique movies. 
The rating column is the outcome,y, which we train the model to predict. 



```{r Table2 ,echo=FALSE}
head_edx <- rbind((lapply(edx, class)), head(edx)) 
head_edx %>% 
  kable(caption = "The first five rows of the edx data set", align = 'cccclc', booktabs = T,
        format = "latex", linesep = "") %>%
  row_spec(1:7, color =  "#41729F", bold = F) %>%
  kable_styling("hover",full_width = FALSE, position = "center", latex_options = c("scale_down", "hold_position"), font_size = 12)
``` 
\ 

### 1.2.2 validation dataset:
The validation dataset has nearly the same characteristic as the edx dataset and is summarized as follows:
 
```{r Table3, echo=FALSE}
###### Table 3: Validation dataset summary 
Validation_Dataset_summary <- data.frame(rows_number = nrow(validation),
                                  columns_number = ncol(validation),
                                  users_number= n_distinct(validation$userId),
                                  movies_number = n_distinct(validation$movieId),
                                  average_rating = round(mean(validation$rating),3),
                                  genres_number = n_distinct(validation$genres),
                                  first_rating_Date = as.Date(as.POSIXct(min(validation$timestamp), origin = "1970-01-01")),
                                  last_rating_date = as.Date(as.POSIXct(max(validation$timestamp), origin = "1970-01-01")))
Validation_Dataset_summary %>% 
  kable(caption = " Validation data set summary", align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  row_spec(1:6, color =  "#41729F", bold = F) %>%
  kable_styling("hover",full_width = FALSE, position = "center", latex_options = c("scale_down", "hold_position"), font_size = 12)
```

\ 

# 1.3.Analysing and exploration of data
A brief synopsis of each feature in the edx dataset is presented in the following. 

##    1.3.1 Investigation of the variable "rating"

Rating, as a statistics variable, is an ordinary scale of numbers. Practically rating is an evaluation number given to the movie by the users i.e. rating: (0.5 ~5, 0.5) where 5 is the best.
An illustrative figure below clarifies the portion of each rating from the total number of ratings that users give to movies, whatever the movie.

+ The overall average rating in the edx dataset was `r round(mean(edx$rating), 2)` \   
+ The top 3 ratings from most to least are  4, 3, 5.\   
+ Users desire to rate movies more positively than negatively.\   
+ The histogram shows that the half-star ratings are less common than whole star ratings.\    

 
```{r Figure1 , echo=FALSE, fig.cap="Overall rating distribution in Edx dataset"}
mu_rating <- mean(edx$rating)
edx %>%
ggplot(aes(x= rating)) +
  geom_histogram( bins = 40, color = "#41729F") +
  theme_hc() + 
  scale_x_continuous(breaks=seq(0, 5, by= 0.5)) +
  geom_vline(xintercept = mu_rating,  colour = "red")+
  labs(x="Rating", y="Number of ratings") + set_theme
```
 
##    1.3.2 Investigation of Movie
The Edx dataset contains a total of 10,677 movies, each represented by a movieId.

+ Some movies are rated more than other, and the average number of rating is 843.   
+ Approximate 20 % number of movies have a number of ratings more than the average, which represents approximately 85% of ratings.   
+ The average movie rating tends to increase when the number of rating increases. 

\ 
The histogram below depicts the number of ratings by a movie. \ 

```{r Figure2, echo=FALSE , fig.cap="Number of ratings by movies in Edx Dataset"}
###### Figure 2: Histogram of number of ratings by movies in Edx Dataset
edx %>% group_by(movieId) %>%
  summarize(num_movie_rating = n(), 
            mu_movies = mean(rating),
            sd_movies = sd(rating)) %>% ggplot(aes(x = num_movie_rating))+
  geom_histogram(bins = 40, color = "#41729F")+
  theme_hc() +
  scale_x_log10()+
  ggtitle("Number of ratings by movie") +
  labs(x="Number of Movies",
       y="Number of ratings") +
  geom_vline(aes(xintercept = mean(num_movie_rating)), color = "red")+
 set_theme
```


The histogram beneath depicts the distribution of movies based on an average rating. \  

```{r Figure3, echo=FALSE, fig.cap="Movie distribution by average rating in Edx Dataset"}
edx %>% group_by(movieId) %>%
  summarise(movie_ave_rating = sum(rating)/n()) %>%
  ggplot(aes(movie_ave_rating)) +
  geom_histogram(bins=30, color = I("#41729F")) +
  theme_hc() +
  scale_x_continuous(breaks=seq(0, 5, by= 0.5)) +
   ggtitle("Movies distribution by average rating ") +
  labs(x="Average Rating",
       y="Number of movies")+
  set_theme
```

## 1.3.3	Investigation of User 

The edx dataset contains 69,878 individuals identified by userId. 

+ 30 % of users contribute 70 % of ratings in the whole edx dataset.  
+ Some users rated very few movies and their opinion may bias the prediction results.  
+ The average user rating tends to increase when the number of rating increases. \ 
 
The number of ratings by a user is represented by the histogram below \ 

```{r Figure4, echo=FALSE, fig.cap="Number of ratings by users in the edx dataset"}
edx %>% group_by(userId) %>%
  summarize(num_user_rating = n(),
            mu_user = mean(rating),
            sd_user = sd(rating)) %>% 
  ggplot(aes(x = num_user_rating))+
  geom_histogram(bins = 40, color = "#41729F")+
  theme_hc() +
  scale_x_log10()+
  labs(x = "Number of Users",
       y = "Number of rating")+
  geom_vline(aes(xintercept = mean(num_user_rating)), color = "red")+
  set_theme
```

On the other hand, the following histogram represents users distribution by average rating.  

```{r Figure5, echo=FALSE, fig.cap="Users distribution by average rating in the edx dataset"}
edx %>% group_by(userId) %>%
  summarise(user_ave_rating = sum(rating)/n()) %>%
  ggplot(aes(user_ave_rating)) +
  geom_histogram(bins=30, color = I("#41729F")) +
  theme_hc() +
  scale_x_continuous(breaks=seq(0, 5, by= 0.5)) +
  labs(x="Average Rating",
       y="Number of Users")+
  set_theme
```

## 1.3.4	  Investigation of the effect of Time on the average rating 

Time is recorded as the UNIX timestamp, which is just several seconds between a certain date and the Unix Epoch. This count begins on January 1st, 1970, at UTC, with the Unix Epoch. 

There is some evidence about the time effect on rating average, but this effect is not strong.\ 
The figure below shows the average movie ratings by month

```{r Figure6, echo=FALSE, fig.cap="Average ratings by time/month in Edx Dataset"}
###### Figure 6: Smooth curve of average ratings by time/month in Edx Dataset
edx %>% 
  mutate(date = round_date(as_datetime(timestamp), unit = "month")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth(color = "#41729F") +
  theme_hc() +
  labs(x = "Time, unit: month ",
       y = "Mean Rating")+
  set_theme
```

## 1.3.5	 Genres Exploration: 
A movie could be classified into one or more genres; there are 20 levels of genre.

+ The number of ratings varies per genre.\    
+ The rating average for genres are Converging, although the number of ratings varies.\    
+ The genres only slightly affect movie ratings.\ 


```{r Figure7, echo=FALSE, fig.cap="Number of ratings by genre in the edx dataset"}
genres_summarize <- edx %>% separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize( num_movie_per_genres = n(), avg_movie_per_genres = mean(rating)) %>%
  arrange(desc(num_movie_per_genres))

genres_summarize %>%
  ggplot(aes(num_movie_per_genres,reorder(genres, num_movie_per_genres),  fill= num_movie_per_genres)) +
  geom_bar(stat = "identity",  color="#41729F", fill="#808080") + coord_flip() +
  scale_fill_distiller(palette = "#41729F")+
  labs(y = "Genres Type",
       x = "Number of ratings")+
  theme_hc()+
  set_theme +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = 'none')
 
```
Figure 7 illustrates the number of movies per genre. On the other hand, we can see the average of ratings per genre as shown in figure 8.

```{r Figure8, echo=FALSE, fig.cap="Distribution of rating average per genre in the edx dataset"}
genres_summarize %>%
  ggplot(aes(avg_movie_per_genres,reorder(genres, avg_movie_per_genres), fill= avg_movie_per_genres)) +
  geom_bar(stat = "identity", color="#41729F", fill="#808080") + coord_flip() +
  geom_smooth(color = "#41729F")+
  scale_fill_distiller(palette = "#41729F") + 
  labs(y = "Genre Type",
       x = "Average of rating")+
  theme_hc()+
  set_theme +
  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = 'none')
```

\   
We can see the relationship  between numbers of ratings and rating average as shown in figure9.

```{r Figure9, echo=FALSE, fig.cap="Relation between Number of Rating vs Mean Rating for Genre"}
edx%>%
  group_by(genres)%>%
  summarise(num_movie_per_genres =n(),avg_rating_genres =mean(rating))%>%
  ggplot(aes(x = num_movie_per_genres, y= avg_rating_genres))+
  scale_x_log10()+
  geom_point()+
  geom_smooth(color = "#41729F")+
  labs(x = "Number of Ratings",
       y = "Mean Ratings")+
  theme_hc()+
  set_theme 
```

\newpage
# 2.	Analysis and modelling  approach

We'll discuss the modeling technique we used to build our models in this section, as well as the metric we used to evaluate model performance.

## 2.1	Model performance evaluation

We'll use root mean squared error (RMSE) as our loss function to compare the performance of our models.
The root mean square error (RMSE) is the difference between the anticipated ratings generated from the model and the actual ratings in the test set.\    
$y_{u,i}$ is the actual rating supplied by user $i$ for movie $u$, $\hat{y}_{u,i}$ is the projected rating for the same, and N is the total number of user/movie pairings in the formula below.  
$$RMSE = \sqrt{\frac{1}{N}\sum_{u,i}\left(\hat{y}_{u,i}-y_{u,i}\right)^2}$$  
 
## 2.2	Constructing and developing baseline Model

Genres and Time do not contribute much information to the data, according to the analysis mentioned above. As a result, I'll examine the movie impact and the user effect to start building a baseline prediction model.

Applying the same rating to all movies is the easiest for forecasting ratings. The real user $u$ rating for movie $i$, $Y_{u,i}$, is the sum of this "true" rating, $\mu$, and the independent errors sampled for the same distribution, $\epsilon_{u,i}$.

$$Y_{u,i}=\mu+\epsilon_{u,i}$$ 

## 2.3 Movie effects

Building on this effect will therefore increase the precision of the prediction as it is known that some movies are normally higher than others. This means perhaps a further enhancement to our model by taking the effect of the film on the rating of $b_i$ into account.  

$$Y_{u,i}=\mu+b_i+\epsilon_{u,i}$$ 

The $\hat{b}_i$, the lowest film estimate, may come from an average of $Y_{u,i}-\hat{\mu}$ for every $i$ movie and, consequently, the following format was utilized in order to take movie effects into account 

$$\hat{b}_{i}=mean\left(\hat{y}_{u,i}-\hat{\mu}\right)$$  

## 2.4 Movie and user effects

Some users are more active in rating films than others hence the algorithm was further refined for the purpose of adjusting for the user impacts ($b_u$). 
$$Y_{u,i}=\mu+b_i+b_u+\epsilon_{u,i}$$

The least square estimates of the user effect, $\hat{b}_u$, were obtained using the following formulas instead of fitting linear regression models.

$$\hat{b}_{u}=mean\left(\hat{y}_{u,i}-\hat{\mu}-\hat{b}_i\right)$$  

## 2.5 Regularization

Regularization allows for high estimates from small sample sizes to be imposed. The Bayesian method, which shrank predictions, has certain commonalities. The overall concept is to apply a penalty to the square equation we reduce for big $b_i$ values. Therefore it is more difficult to reduce with numerous big ${b_i}$ or $b_u$.

By addressing the less square problem, a more accurate calculation of bu and bi will treat them symmetrically

$$\frac{1}{N}\sum_{u,i}\left(y_{u,i}-\mu-b_i-b_u\right)^2+\lambda\left(\sum_ib_i^2+\sum_ub_u^2\right)$$ 

If $\frac{1}{N}\sum_{u,i}\left(y_{u,i}-\mu-b_i-b_u\right)^2$, tries to discover the ratings of both $b_u$'s and $b_i$'s. The $\lambda\left(\sum_ib_i^2+\sum_ub_u^2\right)$ regulation term prevents overfitting by penalizing parameter magnitudes. The method of the stochastic gradient descent, which is an item of the matrix recommender engine, may handle this least-square issue very rapidly.

We utilized cross-validation to get the optimal $\lambda$, and we can prove that with calculus the $b_i$ or $b_u$ variables that minimize this equation are

$$\hat{b}_i\left(\lambda\right)=\frac{1}{\lambda+n_i}\sum_{u=1}^{n_i}\left(Y_{u,i}-\hat{\mu}\right)$$ 

$$\hat{b}_u\left(\lambda\right)=\frac{1}{\lambda+n_i}\sum_{u=1}^{n_i}\left(Y_{u,i}-\hat{\mu}- \hat{b}_i\right)$$ 

## 2.6 Matrix factorization

In recommender systems, matrix factorization is a type of collaborative filtering technique. The user-item interaction matrix is decomposed into the product of two smaller dimensionality rectangular matrices by matric factorization procedures. Because of its success, this family of algorithms became well-known during the Netflix prize challenge, as recounted by Simon Funk in a 2006 [blog post](https://en.wikipedia.org/wiki/Matrix factorization (recommender systems)), in which he shared his findings with the scientific community.

We'll use Matrix Factorization in conjunction with stochastic gradient descent in parallel. It develops a Recommender System by Using Parallel Matrix Factorization with the help of the “recosystem” package, which is a R wrapper of the LIBMF library. The main task of the recommender system i.e. [recosystem](https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html) is to predict unknown entries in the rating matrix based on observed values.
\ 

# 3.Result
The primary goal is to create a recommendation system that will minimize the RMSE to less than 0.86490.\
Because the validation dataset was held aside for the final hold-out test, edx has been divided into two datasets: edx train (80%) and edx test (20%). edx train is used to build a variety of models, while edx test is used to evaluate their performance.\

```{r Model1, include=FALSE, echo=FALSE}
##########################################################
#####  Results section: Presents the modelling results
##########################################################
#### In this section we will do the following:
# 1. Split data set into edx_train 80% and edx_test 10% dataset>
# 2. Constructed various models using edx_train and their performances are assessed using edx_test
# 3. Identifying the optimal model
# 4. Final Model (Results) rerun the optimal model using edx dataset as train set, and validation dataset as test set
#### create train and test set from edx dataset
set.seed(1, sample.kind="Rounding")
# edx_test set will be 10% of edx data
edx_test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
edx_train <- edx %>% slice(-edx_test_index)
edx_temp <- edx %>% slice(edx_test_index)
# make sure userId and movieId in test set are also in train set
edx_test <- edx_temp %>% 
  semi_join(edx_train, by = "movieId") %>%
  semi_join(edx_train, by = "userId")
# add rows removed from test set back into train set
removed <- anti_join(edx_temp, edx_test)
edx_train <- rbind(edx_train, removed)
##########################################################
## Define RMSE: residual mean squared error
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
##########################################################
####### Baseline Approach
####### Model 1: use average ratings for all movies
## calculate average movie rating for edx_train data set
mu <- mean(edx_train$rating)
target_rmse <- 0.86490
## calculate rmse for model 1
model1_rmse <- RMSE(edx_test$rating, mu)
rmse_results <- data.frame(Model = "Just the Average",
                           RMSE = model1_rmse)
```

## 3.1 Model 1: BaseLine Model
\   
It's just a model that ignores all the feathers and calculates the average rating. This model serves as a baseline against which we will aim to improve RMSE. \
The RMSE is `r round(model1_rmse,4)` and the average rating is $\mu$= `r round(mu,4)`.

```{r view-rmse1, echo=FALSE}
rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#41729F", bold = T) %>%
  column_spec(2, color =  "#41729F", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```

## 3.2 Model 2: Movie effects Model

We'll add the bias of movies $b_i$ for each movie to the model because the features of a film can influence its ratings.
The average rating for that particular film will differ from the general average rating for all films. We'll determine the RMSE of this model by plotting the movie bias distribution.

```{r Figure10, echo=FALSE}
movie_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

movie_avgs %>%
  ggplot(aes(b_i)) +
  geom_histogram(bins=30, color = I("#41729F")) +
  theme_hc() +
  labs(x="Movie effects (b_i)",
       y="Count",
       caption = "Figure 10: Distribution of movie effect (b_i) edx_train dataset")+
  theme(text = element_text(size=16), plot.background = element_rect(colour= NA, linetype = "solid", fill = NA, size = 1), panel.border = element_rect(colour="black", linetype = "solid", fill=NA), plot.title = element_text(hjust = 0.5, size = 20), plot.caption = element_text(hjust = 0.5, size = 22))
```

```{r Model2,include=FALSE, echo=FALSE}
## calculate the prediction rating for model 2
predict_rating_m2 <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(pred = mu + b_i)
## calculate rmse for model 2
model2_rmse <- RMSE(edx_test$rating,predict_rating_m2$pred)
rmse_results <- rbind(rmse_results,data.frame(Model = "Movie Effect Model",
                           RMSE = round(model2_rmse, 4)))
```

Figure 10 depicts how the estimate of movie effect $\hat{b}_i$ differs significantly across all of the movies in the training dataset. Including the movie effect in the algorithm increased the model's accuracy by `r percent((model1_rmse-model2_rmse)/model1_rmse)`, producing an RMSE of `r round(model2_rmse,4)`, which is still higher than the target.

```{r view-rmse2, echo=FALSE}
rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#41729F", bold = T) %>%
  column_spec(2, color =  "#41729F", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```

## 3.3 Model 3: Movies and Users effects Model

In a manner analogous to the movie effect, a user's characteristics can influence a movie's rating. A user's overall rating for all movies watched could be lower than what other users have rated. \
The user bias ${b_u}$ will be added to the movie effect model, and the RMSE will be calculated. 

```{r Figure11, echo=FALSE}
user_avgs <- edx_train %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
## Figure 11: Distribution of movie effect (b_u)
user_avgs %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins=30, color = I("#41729F")) +
  theme_hc() +
  labs(x="User effects (b_u)" ,
       y="Count",
       caption = "Figure 11: Distribution of movie effect (b_u) in edx_train dataset")+
  theme(text = element_text(size=16), plot.background = element_rect(colour= NA, linetype = "solid", fill = NA, size = 1), panel.border = element_rect(colour="black", linetype = "solid", fill=NA), plot.title = element_text(hjust = 0.5, size = 20), plot.caption = element_text(hjust = 0.5, size = 22))
```

```{r Model3, echo=FALSE}
## calculate the prediction rating for model 3
predict_rating_m3 <- edx_test %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) 
## calculate rmse for model 3
model3_rmse <- RMSE(edx_test$rating,predict_rating_m3$pred)
rmse_results <- rbind(rmse_results,
                          data_frame(Model="Movie and User Effect model",  
                                    RMSE = round(model3_rmse, 4)))
```

The predicted effect of user $\hat{b}_u$ building on the movie effects model is shown in Figure 11. It was clear that correcting for user effects improved the algorithm's accuracy. Adjusting for both the movie and user impacts reduces the RMSE by `r percent((model1_rmse-model3_rmse)/model1_rmse)` when compared to the baseline model.

```{r view-rmse3, echo=FALSE}
rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#41729F", bold = T) %>%
  column_spec(2, color =  "#41729F", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```

## 3.4 Model 4: Regularization of movie and user effects

The approach of regularization was being applied with regard to movie and user effects by applying a higher penalty to smaller sample estimations. So we're going to employ $\lambda$ parameter.

During our data analysis, we discovered that some people are more engaged in movie reviews than others. Some people have only rated a few films. Some films, on the other hand, have received extremely few ratings.
We should not put our faith in these erratic estimates. Furthermore, RMSE is susceptible to huge mistakes.
As a result, we must include a penalty word to devalue such an effect.

```{r Figure12, echo=FALSE}
## calculate optimal tuning parameter (Lambda) using k fold cross validation
lambdas <- seq(0, 10, 0.25)
## calculate the best value of lambdas that return minimum RMSE value
set.seed(21, sample.kind = "Rounding")
## For each lambda,find b_i & b_u followed by rating prediction
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(edx_train$rating)
  
  b_i <- edx_train %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx_train %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- edx_test %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(edx_test$rating,predicted_ratings))
})
## plot lambadas
qplot(lambdas, rmses)  +
  labs(x="Lambda",
       y="RMSE",
       caption = "Figure 12: Selecting the tuning parameter in edx_train dataset")+
  theme(text = element_text(size=16), plot.background = element_rect(colour= NA, linetype = "solid", fill = NA, size = 1), panel.border = element_rect(colour="black", linetype = "solid", fill=NA), plot.title = element_text(hjust = 0.5, size = 20), plot.caption = element_text(hjust = 0.5, size = 22))
```

```{r Model4, echo=FALSE}
## get the optimal value for lambda
lambda <- lambdas[which.min(rmses)]
## calculate the regular movie reg_b_i with the optimal lambda
reg_movie_avgs <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(reg_b_i = sum(rating - mu)/(n()+lambda), n_i = n())
  
## calculate the regular user reg_b_u with the optimal lambda
reg_user_avgs <- edx_train %>% 
  left_join(reg_movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(reg_b_u = sum(rating - mu - reg_b_i)/(n()+lambda), n_u = n())
## calculate the prediction rating for model 4
reg_predicted_ratings <- edx_test %>% 
  left_join(reg_movie_avgs, by='movieId') %>%
  left_join(reg_user_avgs, by='userId') %>%
  mutate(pred = mu + reg_b_i + reg_b_u) %>% 
  .$pred
## calculate rmse for model 4
model4_rmse <- RMSE(edx_test$rating,reg_predicted_ratings)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Regularized Movie and User Effect Model",  
                                     RMSE = round(model4_rmse, 4)))
```

The RMSE delivered across each of the lambda values evaluated is shown in the graph above. The ideal value for $lambda$ was `r round(lambda,4)`, which reduced the RMSE to `r round(model4_rmse,4)`, which was just enough to beat the project's target RMSE. This resulted in a total improvement in baseline accuracy of `r percent((model1_rmse-model4_rmse)/model1_rmse)`.

```{r , echo=FALSE}
rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#41729F", bold = T) %>%
  column_spec(2, color =  "#41729F", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```


```{r view-rmse4, echo=FALSE}
####### Model 5: Matrix Factorization based on the residuals of the best model (Model 4: Regularized Movie and user Effect Model)
####### Calculating the residuals of edx_train from model 4
residual_edx <- edx_train %>% 
  left_join(reg_movie_avgs, by = "movieId") %>%
  left_join(reg_user_avgs, by = "userId") %>%
  mutate(residual = rating - mu - reg_b_i - reg_b_u) %>%
  select(userId, movieId, residual)
#edhead(residual_edx)
#######  Use the recosystem package to perform the matrix factorization
## make matrix from residual and edx_test
residual_mf <- as.matrix(residual_edx)
edx_test_mf <- edx_test %>% 
  select(userId, movieId, rating)
edx_test_mf <- as.matrix(edx_test_mf)
## write residual_mf and edx_test_mf table on disk
write.table(residual_mf , file = "trainset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)
write.table(edx_test_mf, file = "testset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)
## use data_file() to specify a data set from a file in the hard disk.
train_set <- data_file("trainset.txt")
test_set <- data_file("testset.txt")
## build a recommender object
r <-Reco()
# tuning training set
# Note: running this code will take along time ~30 minits
opts <- r$tune(train_set, opts = list(dim = c(10, 20, 30), lrate = c(0.1, 0.2),
                                      costp_l1 = 0, costq_l1 = 0,
                                      nthread = 1, niter = 10))
# training the recommender model
r$train(train_set, opts = c(opts$min, nthread = 1, niter = 20))
# Making prediction on validation set and calculating RMSE:
pred_file <- tempfile()
r$predict(test_set, out_file(pred_file)) 
predicted_residuals_mf <- scan(pred_file)
predicted_ratings_mf <- reg_predicted_ratings + predicted_residuals_mf
## calculate rmse for model 5
model5_rmse <- RMSE(edx_test$rating, predicted_ratings_mf)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="Matrix Factorization",  
                                     RMSE = round(model5_rmse, 4)))
```
## 3.5 Model 5: Matrix Factorization
The following results may be achieved by following the necessary process for recosystem library provided on the site. \ 
We get RMSE equal to `r round(model5_rmse,4)` by applying the  Matrix factorization method. By calculating the decrease in the percentage of RMSE we can observe a drop of more than `r percent((model1_rmse-model5_rmse)/model1_rmse)` % in the matrix factorization technique in comparison with the basic model.


```{r , echo=FALSE}
rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#41729F", bold = T) %>%
  column_spec(2, color =  "#41729F", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```

```{r , echo=FALSE}

## calculation of rmse for (Matrix Factorization) Model
##########################################################################################
##### Final Model: Use Model 5 (Matrix Factorization)  to assessed on validation set #####
##########################################################################################
## so will be used to all edx set as train set, and validation set as test set in model 5
####### calculate optimal tuning parameter (Lambda) using k fold cross validation
set.seed(1, sample.kind = "Rounding")
## calculation of average movie rating for edx data set
f_mu <- mean(edx$rating)
## calculation of the best value of lambdas that return minimum RMSE value
f_lambdas <- seq(0, 10, 0.25)
f_rmses <- sapply(f_lambdas, function(l){
  
  mu <- mean(edx$rating)
  
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred
  
  return(RMSE(validation$rating,predicted_ratings))
})
## estimation of the optimal value for lambda
f_lambda <- f_lambdas[which.min(f_rmses)]
f_lambda
qplot(f_lambdas, f_rmses)
## calculation of the final regular movie f_b_i with the optimal lambda
final_movie_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(f_b_i = sum(rating - f_mu)/(n()+f_lambda), n_i = n())
## calculation of the final regular user f_b_u with the optimal lambda
final_user_avgs <- edx %>% 
  left_join(final_movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(f_b_u = sum(rating - f_mu - f_b_i)/(n()+f_lambda), n_u = n())
## calculation of the regular prediction rating using validation dataset
final_reg_predicted_ratings <- validation %>% 
  left_join(final_movie_avgs, by='movieId') %>%
  left_join(final_user_avgs, by='userId') %>%
  mutate(pred = f_mu + f_b_i + f_b_u) %>% 
  .$pred
## calculation of RMSE for Regularized Movie and User Effect Model , RMSE = 0.864817
#final_reg_rmse <- RMSE(validation$rating,final_reg_predicted_ratings)
#final_reg_rmse
####### Calculation of the residuals of edx data set
final_residual_edx <- edx %>% 
  left_join(final_movie_avgs, by = "movieId") %>%
  left_join(final_user_avgs, by = "userId") %>%
  mutate(residual = rating - f_mu - f_b_i - f_b_u) %>%
  select(userId, movieId, residual)
#head(final_residual_edx)
#######  Using the recosystem package to perform the matrix factorization
## make matrix from residual and validation set
final_residual_mf <- as.matrix(final_residual_edx)
validation_mf <- validation %>% 
  select(userId, movieId, rating)
validation_mf <- as.matrix(validation_mf)
## writing of final_residual_mf and validation_mf table on disk
write.table(final_residual_mf , file = "final_trainset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)
write.table(validation_mf, file = "final_testset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)
## using data_file() to specify a data set from a file in the hard disk.
final_train_set <- data_file("final_trainset.txt")
final_test_set <- data_file("final_testset.txt")
## build a recommender object
f_r <-Reco()
# tuning training set
# Note: running this code will take along time ~30 minits
f_opts <- f_r$tune(final_train_set, opts = list(dim = c(10, 20, 30), lrate = c(0.1, 0.2),
                                      costp_l1 = 0, costq_l1 = 0,
                                      nthread = 1, niter = 10))
# training the recommended model
f_r$train(final_train_set, opts = c(f_opts$min, nthread = 1, niter = 20))
# Making prediction on validation set and calculating RMSE:
final_pred_file <- tempfile()
f_r$predict(final_test_set, out_file(final_pred_file)) 
final_predicted_residuals_mf <- scan(final_pred_file)
final_predicted_ratings_mf <- final_reg_predicted_ratings + final_predicted_residuals_mf
## calculate rmse for final model (model 5: Matrix Factorization)
final_rmse <- RMSE(validation$rating, final_predicted_ratings_mf)
final_rmse_results <- data.frame(Model = "Best Model: Matrix Factorization",
                                 RMSE = round(final_rmse, 4))
```

## 3.6 Final Model
The above sections create models that are trained using edx_test, a subset of the edx dataset. This results in that Matrix factorization with an RMSE of `r round(model5_rmse,4)` is the top-ranked model. The final model, therefore, follows the Matrix factorization approach.
The ultimate model is based on "edx_dataset" and evaluated using the "validation" dataset.  RMSE of `r round(final_rmse,4)` which is an improvement of `r percent((model1_rmse-final_rmse)/model1_rmse)` compared to a baseline model was achieved by the last hold-out test in the validation dataset. As may be seen in the following table.

```{r , echo=FALSE}
final_rmse_results %>% 
  kable(align = 'c', booktabs = T,
        format = "latex", linesep = "") %>%
  column_spec(1, color =  "#41729F", bold = T) %>%
  column_spec(2, color =  "#41729F", bold = T) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = c("hold_position"))
```


# 4. Conclusion 

This study goes through a few approaches for building recommendation systems with a residual mean square error of less than 0.86490. When the model trained on edx and evaluated on validation, the highest performing model is matrix factorization, which has an RMSE of `r round(final_rmse,4)`.This improves the RMSE's model  by around `r percent((model1_rmse-final_rmse)/model1_rmse)` on the first model.
While this algorithm built here fulfilled the goal of the study, there is still a significant error loss suggesting that the accuracy of the recommendation systems may still be improved.
Finally, a particularly strong method for the recommendation systems seems to be matrix factorization. Those interested in an enhanced way to constructing systems that suggest the recosystem package should consult. It helps with the development of matrix factorisation in huge quantities of data.

\newpage
# 5. References
[1] “Introduction to Data Science - Data Analysis and Prediction Algorithms with R”, Dr. Rafael A. Irizarry [link](https://rafalab.github.io/dsbook/)

[2] "R Markdown: The Definitive Guide", Yihui Xie, J. J. Allaire, Garrett Grolemund, 2019-06-03 [link](https://bookdown.org/yihui/rmarkdown/)

[3] “Recommender System Using Parallel Matrix Factorization”, Yixuan Qiu, 2021-01-09.  [link](https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html)

[4] "E. Winning the Netflix Prize: A Summary", Chen, 2020/10/15. [link](http://blog.echen.me/2011/10/24/winning-the-netflixprize-a-summary/)


\newpage


# 6.Appendices
## Appendix A: 
### A.1 MovieLens Project Instructions

The submission for the MovieLens project will be three files: a report in the form of an 
Rmd file, a report in the form of a PDF document knit from your Rmd file, and an R script that 
generates your predicted movie ratings and calculates RMSE. The R script should contain all of 
the code and comments for your project. 
Your grade for the project will be based on two factors:

  1.Your report and script (75%) \
  2.The RMSE returned by testing your algorithm on the validation set  (the final hold-out test set) (25%)
  
Note that to receive full marks on this project, you may not simply copy code from other 
courses in the course series and be done with your analysis. Your work on this project 
needs to build on code that is already provided.
Please note that once you submit your project, you will not be able to make changes to your
submission.

#### A.1.1 Report and Script (75%) \
Your report and script will be graded by your peers, based on a rubric defined by the course 
staff. Each submission will be graded by three peers and the median grade will be awarded. 
To receive your grade, you must review and grade the submissions of five of your fellow 
learners after submitting your own. This will give you the chance to learn from your peers.

Please pay attention to the due dates listed! The project submission is due before the end of 
the course to allow time for peer grading. 
Also note that you must grade the reports of your peers by the course close date in order to 
receive your grade.


#### A.1.2 RMSE (25%) \
Your movie rating predictions will be compared to the true ratings in the validation set 
(the final hold-out test set) using RMSE. 
Be sure that your report includes the RMSE and that your R script outputs the RMSE.
 
Note that to receive full marks on this project, you may not simply copy code from other 
courses in the course series and be done with your analysis. 
Your work on this project needs to build on code that is already provided.
 
IMPORTANT: Make sure you do NOT use the validation set (the final hold-out test set) to 
train your algorithm. 
The the final hold-out test set should ONLY be used to test your final algorithm. 
The final hold-out test set should only be used at the end of your project with your final model.
It may not be used to test the RMSE of multiple models during model development. 
You should split the edx data into a training and test set or use cross-validation.


#### A.1.3 Honor Code \
You are welcome to discuss your project with others, but all submitted work must be your own. 
Your participation in this course is governed by the terms of the edX Honor Code. 
If your project is found to violate the terms of the honor code, you will receive a zero on the 
project, may be unenrolled from the course, and will be ineligible for a certificate.


#### A.1.4 Project Due Date \
Submissions for the Movielens project are due one week before course close, on March 9, 2022, at 23:59 UTC. This allows time for peer grading to occur! Peer grades are due at course close, on March 16, 2022, at 23:59 UTC.

#### A.1.5 Peer Feedback \
You are strongly encouraged to give your peers thoughtful, specific written feedback in addition
to the numerical grades in the rubic. Think about the type of feedback that would help you 
improve your work and offer that type of feedback to your fellow learners.
 
If you feel your report was not fairly graded by your peers, you may report it in the 
discussion forum to ask for staff review of the report. 

\newpage 

## Appendix B:
### B.1 MovieLens Project Submission

#### B.2.1 Your Response \ 
due Jul 29, 2021 02:59 EEST (in 2 weeks, 3 days)

Enter your response to the prompt. You can save your progress and return to complete your response at any time before the due date (Thursday, Jul 29, 2021 02:59 EEST). After you submit your response, you cannot edit it.

#### B.2.2 The prompt for this section \ 
Your submission for this project is three files:  \
**1.Your report in Rmd format**\
**2.Your report in PDF format (knit from your Rmd file)**\
**3.A script in R format that generates your predicted movie ratings and RMSE score (should contain all code and comments for your project)** \

You may upload the three files directly to the edX platform or submit a GitHub link in the text response box below.

To upload and submit your files press the "Choose Files" button, select three files at once (using the control key on a Windows machine or command key on a Mac) and press "Choose," type a description for each (PDF, Rmd, R), and then press the "Upload files" button. If uploading files, we recommend also providing a link to a GitHub repository containing the three files above in case there is a problem with the upload process.

Note that when downloading files for peer assessments, R and Rmd files will be downloaded as txt files by default.\ 

#### B.2.3 MovieLens Grading Rubric \
The following is the grading rubric your peers will be using to evaluate your project. There are also opportunities for your peers to provide written feedback as well (required for some categories and optional for others). You are encouraged to give thoughtful, specific written feedback to your peers whenever possible (i.e., more than just "good job" or "not enough detail").

Note that to receive full marks on this project, you may not simply copy code from other courses in the course series and be done with your analysis. Your work on this project needs to build on code that is already provided.

After you submit your project, please check immediately after submitting to make sure that all files were correctly uploaded. Occasionally, there are file upload failures, and it's easiest to fix if these are caught early.

#### B.2.3.a Files (10 points possible)\
The appropriate files are submitted in the correct formats: a report in both PDF and Rmd format and an R script in R format.

+ 0 points: No files provided AND/OR the files provided appear to violate the edX Honor Code.\
+ 3 points: Multiple requested files are missing and/or not in the correct formats.\
+ 5 points: One file is missing and/or not in the correct format.\
+ 10 points: All 3 files were submitted in the requested formats.

#### B.2.3.b Report (40 points possible) \
The report documents the analysis and presents the findings, along with supporting statistics and figures. The report must be written in English and uploaded. The report should be written assuming that the reader is not familiar with the project or the data. The report must include the RMSE generated. The report must include at least the following sections:\
**1. an introduction/overview/executive summary section that describes the dataset and summarizes the goal of the project and key steps that were performed.** \
**2. a methods/analysis section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach.** \
**3. a results section that presents the modeling results and discusses the model performance.** \
**4. a conclusion section that gives a brief summary of the report, its limitations and future work.** 

+ 0 points: The report is either not uploaded or contains very minimal information AND/OR the report appears to violate the edX Honor Code.\
+ 10 points: Multiple required sections of the report are missing.
+ 15 points: The methods/analysis or the results section of the report is missing or missing significant supporting details. Other sections of the report are present.\
+ 20 points: The introduction/overview or the conclusion section of the report is missing, not well-presented or not consistent with the content.\
+ 20 points: The report includes all required sections, but the report is significantly difficult to follow or missing supporting detail in multiple sections.\
+ 25 points: The report includes all required sections, but the report is difficult to follow or missing supporting detail in one section.\
+ 30 points: The report includes all required sections and is well-drafted and easy to follow, but with minor flaws in multiple sections.\
+ 35 points: The report includes all required sections and is easy to follow, but with minor flaws in one section.\
+ 40 points: The report includes all required sections, is easy to follow with good supporting detail throughout, and is insightful and innovative.

#### B.2.3.c Code (25 points) \
The code in the R script should should be well-commented and easy to follow. The code provided in the R script should contain all of the code and comments for your project. You are not required to run the code provided (although you may if you wish), but you should visually inspect it.

+ 0 points: No code provided AND/OR the code appears to violate the edX Honor Code.\
+ 10 points: Code appears that it would not run/is very difficult to follow or interpret AND/OR is not consistent with the report.\
+ 15 points: Code appears that it would run without throwing errors, can be followed, is at least mostly consistent with the report, but has no comments or explanation.\
+ 15 points: Code is simply a copy of code provided in previous courses in the series without expanding on it, but is otherwise well-commented.\
+ 20 points: Code appears that it would run without throwing errors, can be followed, is largely consistent with the report, but without sufficient comments or explanations.\
+ 25 points: Code is easy to follow, is consistent with the report, and is well-commented.

#### B.2.4 RMSE (25 points) \
Provide the appropriate score given the reported RMSE. Please be sure not to use the validation set (the final hold-out test set) for training or regularization - you should create an additional partition of training and test sets from the provided edx dataset to experiment with multiple parameters or use cross-validation.

+ 0 points: No RMSE reported AND/OR code used to generate the RMSE appears to violate the edX Honor Code.
+ 5 points: RMSE >= 0.90000 AND/OR the reported RMSE is the result of overtraining (validation set - the final hold-out test set - ratings used for anything except reporting the final RMSE value) AND/OR the reported RMSE is the result of simply copying and running code provided in previous courses in the series.\
+ 10 points: 0.86550 <= RMSE <= 0.89999\
+ 15 points: 0.86500 <= RMSE <= 0.86549\
+ 20 points: 0.86490 <= RMSE <= 0.86499\
+ 25 points: RMSE < 0.86490\


